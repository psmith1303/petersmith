+++
title = "Peer Evaluations"
date = "2011-05-02T23:14:00+12:00"
categories = ["BUSINESS 101"]
slug = "peer-evaluations"
draft = "False"
+++
Our BUSINESS 101 course (aka Business and Enterprise) is based on
[Team-based learning](http://www.teambasedlearning.org/) and has an
element of peer evaluation.This is based on the [Michaelson (percentage)
method](http://tblc.camp9.org/resources/documents/tbl%20-%202%20methods_peer%20eval%20scores.pdf).

We have just completed a practice run through and the results are quite
interesting (but not unexpected).

![Practice Peer Evaluation results for BUSINESS 101, Semester 1, 2011](/images/2011S1B101PeerEvals-practice.png)

Approximately half the class completed the practice team review.

The distribution looks pretty normal (as many are above 100 as below
100). The tails are driven (in this practice session) by either a) teams
were only one or two people provided feedback, or b) where people are
actually no longer in the team (at the bottom).

No doubt there will be some (understandable) angst from those who are
significantly below the 100, especially if the feedback from their peers
does not articulate well why they scored lowly. For these people, I'm
always happy to facilitate a discussion with their team so they can
figure out what they need to improve to get a better score from their
team-members.

In the finial (actual) peer evaluation, I'd expect the distribution to
tighten up somewhat.

